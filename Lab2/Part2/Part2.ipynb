{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : Priyanka Manoj Naik - 50248591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Search phrase =', 'immigration')\n",
      "('search limit (start/stop):', datetime.datetime(2018, 4, 4, 23, 59, 59))\n",
      "('search limit (start/stop):', datetime.datetime(2018, 4, 3, 23, 59, 59))\n",
      "('max id (starting point) =', 981682829506605067)\n",
      "('since id (ending point) =', 981320441649229824)\n",
      "('count =', 1)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 2)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 3)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 4)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 5)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 6)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 7)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 8)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 9)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 10)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 11)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 12)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 13)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 14)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 15)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 16)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 17)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 18)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 19)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 20)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 21)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 22)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 23)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 24)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 25)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 26)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 27)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 28)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 29)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 30)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 31)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 32)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 33)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 34)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 35)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 36)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 37)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 38)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 39)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 40)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 41)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 42)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 43)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 44)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 45)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 46)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 47)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 48)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 49)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 50)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 51)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 52)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 53)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 54)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 55)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 56)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 57)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 58)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 59)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 60)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 61)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 62)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 63)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 64)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 65)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 66)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 67)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 68)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 69)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 70)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 71)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 72)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 73)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 74)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 75)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 76)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 77)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 78)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 79)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 80)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 81)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 82)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 83)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 84)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 85)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 86)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 87)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 88)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 89)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 90)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 91)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 92)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 93)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 94)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 95)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 96)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 97)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 98)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 99)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 100)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 101)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 102)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 103)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 104)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 105)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 106)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 107)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 108)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 109)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 110)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 111)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 112)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 113)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 114)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 115)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 116)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 117)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 118)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 119)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 120)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 121)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 122)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 123)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 124)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 125)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 126)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 127)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 128)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 129)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 130)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 131)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 132)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 133)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 134)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 135)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 136)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 137)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 138)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 139)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 140)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 141)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 142)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 143)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 144)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 145)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 146)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 147)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 148)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 149)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 150)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 151)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 152)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 153)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 154)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 155)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 156)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 157)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 158)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 159)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 160)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 161)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 162)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 163)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 164)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 165)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 166)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 167)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 168)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 169)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 170)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 171)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 172)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 173)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 174)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 175)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 176)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 177)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 178)\n",
      "('found', 92, 'tweets')\n",
      "exception raised, waiting 15 minutes\n",
      "('(until:', datetime.datetime(2018, 4, 5, 13, 59, 58, 466312), ')')\n",
      "('count =', 179)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 180)\n",
      "('found', 98, 'tweets')\n",
      "('found', 2, 'tweets')\n",
      "('count =', 181)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 182)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 183)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 184)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 185)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 186)\n",
      "('found', 99, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('count =', 187)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 188)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('found', 100, 'tweets')\n",
      "('count =', 189)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 190)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 191)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 192)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 193)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 194)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 195)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 196)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 197)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 198)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 199)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 200)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 201)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 202)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 203)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 204)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 205)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 206)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 207)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 208)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 209)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 210)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 211)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 212)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 213)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 214)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 215)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 216)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 217)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 218)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 219)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 220)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 221)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 222)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 223)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 224)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 225)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 226)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 227)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 228)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 229)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 230)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 231)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 232)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 233)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 234)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 235)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 236)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 237)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 238)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 239)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 240)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 241)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 242)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 243)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 244)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 245)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 246)\n",
      "('found', 93, 'tweets')\n",
      "('found', 7, 'tweets')\n",
      "('count =', 247)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 248)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 249)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 250)\n",
      "('found', 98, 'tweets')\n",
      "('found', 2, 'tweets')\n",
      "('count =', 251)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 252)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 253)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 254)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 255)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 256)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 257)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 258)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 259)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 260)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 261)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 262)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 263)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 264)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 265)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 266)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 267)\n",
      "('found', 97, 'tweets')\n",
      "('found', 3, 'tweets')\n",
      "('count =', 268)\n",
      "('found', 77, 'tweets')\n",
      "('found', 18, 'tweets')\n",
      "('found', 5, 'tweets')\n",
      "('count =', 269)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 270)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 271)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 272)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 273)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 274)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 275)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 276)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 277)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 278)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 279)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 280)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 281)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 282)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 283)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 284)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 285)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 286)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 287)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 288)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 289)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 290)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 291)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 292)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 293)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 294)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 295)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 296)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 297)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 298)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 299)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 300)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 301)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 302)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 303)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 304)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 305)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 306)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 307)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 308)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 309)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 310)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 311)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 312)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 313)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 314)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 315)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 316)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 317)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 318)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 319)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 320)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 321)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 322)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 323)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 324)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 325)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 326)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 327)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 328)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 329)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 330)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 331)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 332)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 333)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 334)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 335)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 336)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 337)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 338)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 339)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 340)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 341)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 342)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 343)\n",
      "('found', 94, 'tweets')\n",
      "('found', 5, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('count =', 344)\n",
      "('found', 93, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('found', 4, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('count =', 345)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 346)\n",
      "exception raised, waiting 15 minutes\n",
      "('(until:', datetime.datetime(2018, 4, 5, 14, 19, 46, 876274), ')')\n",
      "('count =', 347)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 348)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 349)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 350)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 351)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 352)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 353)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 354)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 355)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 356)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 357)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 358)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 359)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 360)\n",
      "('found', 95, 'tweets')\n",
      "('found', 5, 'tweets')\n",
      "('count =', 361)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 362)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 363)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 364)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 365)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 366)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 367)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 368)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 369)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 370)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 371)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 372)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 373)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 374)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('found', 100, 'tweets')\n",
      "('count =', 376)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 377)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 378)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 379)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 380)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 381)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 382)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 383)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 384)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 385)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 386)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 387)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 388)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 389)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 390)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 391)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 392)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 393)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 394)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 395)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 396)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 397)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 398)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 399)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 400)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 401)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 402)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 403)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 404)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 405)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 406)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 407)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 408)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 409)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 410)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 411)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 412)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 413)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 414)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 415)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 416)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 417)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 418)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 419)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 420)\n",
      "('found', 94, 'tweets')\n",
      "('found', 6, 'tweets')\n",
      "('count =', 421)\n",
      "('found', 99, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('count =', 422)\n",
      "('found', 84, 'tweets')\n",
      "('found', 16, 'tweets')\n",
      "('count =', 423)\n",
      "('found', 98, 'tweets')\n",
      "('found', 1, 'tweets')\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('count =', 424)\n",
      "('found', 95, 'tweets')\n",
      "('found', 5, 'tweets')\n",
      "('count =', 425)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 426)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 427)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 428)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 429)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 430)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 431)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 432)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 433)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 434)\n",
      "('found', 98, 'tweets')\n",
      "('found', 2, 'tweets')\n",
      "('count =', 435)\n",
      "('found', 96, 'tweets')\n",
      "('found', 4, 'tweets')\n",
      "('count =', 436)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 437)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 438)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 439)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 440)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 441)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 442)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 443)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 444)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 445)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 446)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 447)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 448)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 449)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 450)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 451)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 452)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 453)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 454)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 455)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 456)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 457)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 458)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 459)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 460)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 461)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 462)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 463)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 464)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 465)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 466)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 467)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 468)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 469)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 470)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 471)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 472)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 473)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 474)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 475)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 476)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 477)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 478)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 479)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 480)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 481)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 482)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 483)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 484)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 485)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 486)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 487)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 488)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 489)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 490)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 491)\n",
      "('found', 100, 'tweets')\n",
      "('count =', 492)\n",
      "('found', 10, 'tweets')\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('count =', 493)\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('count =', 494)\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n",
      "('count =', 495)\n",
      "('found', 0, 'tweets')\n",
      "no tweets found\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Maximum number of empty tweet strings reached - exiting",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Maximum number of empty tweet strings reached - exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankanaik/Library/Python/2.7/lib/python/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "'''Function that loads the twitter API after authorizingthe user.'''\n",
    "def load_api():\n",
    "    consumer_key = \"d9q3JVpPEOhb7pLqlP3Tn7LBw\"\n",
    "    consumer_secret = \"aTZj72mUPrUvZHOofDzOAMp8Ro1FVwHNzEKPZA4H3kww13RhKJ\"\n",
    "    access_token = \"960608039458963456-Zt65NRJyDL4ws97zsOND8lPpLdXweL8\"\n",
    "    access_secret = \"KjFj1n7HryIxm4MmjuF3df5hNXmmT1vecqZGokYfPWkMV\"\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "''' Function that takes in a search string 'query', the maximum\n",
    "    number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "    tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets, since_id=str(since_id), max_id=str(max_id-1), geocode = geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now() + dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id\n",
    "\n",
    "''' Function that gets the ID of a tweet. This ID can\n",
    "    then be used as a 'starting point' from which to\n",
    "    search. The query is required and has been set to\n",
    "    a commonly used word by default. The variable\n",
    "    'days_ago' has been initialized to the maximum amount\n",
    "    we are able to search back in time (9).'''\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'): \n",
    "    if date: # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "    \n",
    "''' Function that appends tweets to a file. '''\n",
    "def write_tweets(tweets, filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "def main():\n",
    "    search_phrases = ['immigration']\n",
    "    time_limit = 1.5 # runtime limit in hours\n",
    "    max_tweets = 100                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    min_days_old, max_days_old = 1, 2          # search limits e.g., from 7 to 8\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "    tweet_files = []\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "\n",
    "        print('Search phrase =', search_phrase)\n",
    "\n",
    "        ''' other variables '''\n",
    "        name = search_phrase.split()[0]\n",
    "        json_file_root = 'TwitterData/' + name\n",
    "        try:\n",
    "            if not os.path.exists(os.path.dirname(json_file_root)):\n",
    "                os.makedirs(os.path.dirname(json_file_root))\n",
    "        except OSError as err:\n",
    "            print(err)\n",
    "        read_IDs = False\n",
    "        \n",
    "        # open a file in which to store the tweets\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "        tweet_files.append(json_file)\n",
    "        if os.path.isfile(json_file):\n",
    "            print('Appending tweets to file named: ',json_file)\n",
    "            read_IDs = True\n",
    "        \n",
    "        # authorize and load the twitter API\n",
    "        api = load_api()\n",
    "        \n",
    "        # set the 'starting point' ID for tweet collection\n",
    "        if read_IDs:\n",
    "            # open the json file and get the latest tweet ID\n",
    "            with open(json_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                print('Searching from the bottom ID in file')\n",
    "        else:\n",
    "            # get the ID of a tweet that is min_days_old\n",
    "            if min_days_old == 0:\n",
    "                max_id = -1\n",
    "            else:\n",
    "                max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "        # set the smallest ID to search for\n",
    "        since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "        print('max id (starting point) =', max_id)\n",
    "        print('since id (ending point) =', since_id)\n",
    "        \n",
    "        ''' tweet gathering loop  '''\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exitcount = 0, 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            print('count =',count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,max_id=max_id, since_id=since_id,geocode=USA)\n",
    "            # write tweets to file in JSON format\n",
    "            if tweets:\n",
    "                write_tweets(tweets, json_file)\n",
    "                exitcount = 0\n",
    "            else:\n",
    "                exitcount += 1\n",
    "                if exitcount == 3:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                    else:\n",
    "                        print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180404\n",
      "20180405\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import sys, os\n",
    "import logging\n",
    "from urllib2 import HTTPError\n",
    "from ConfigParser import SafeConfigParser\n",
    "\n",
    "\n",
    "# helper function to iterate through dates\n",
    "def daterange( start_date, end_date ):\n",
    "    if start_date <= end_date:\n",
    "        for n in range( ( end_date - start_date ).days + 1 ):\n",
    "            yield start_date + datetime.timedelta( n )\n",
    "    else:\n",
    "        for n in range( ( start_date - end_date ).days + 1 ):\n",
    "            yield start_date - datetime.timedelta( n )\n",
    "\n",
    "# helper function to get json into a form I can work with       \n",
    "def convert(input):\n",
    "    if isinstance(input, dict):\n",
    "        return {convert(key): convert(value) for key, value in input.iteritems()}\n",
    "    elif isinstance(input, list):\n",
    "        return [convert(element) for element in input]\n",
    "    elif isinstance(input, unicode):\n",
    "        return input.encode('utf-8')\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "# helpful function to figure out what to name individual JSON files        \n",
    "def getJsonFileName(date, page, json_file_path, query):\n",
    "    json_file_name = \"_\".join([query, date, str(page)])\n",
    "    json_file_name = \"\".join([json_file_name, \".\", 'json'])\n",
    "    json_file_name = \"\".join([json_file_path,json_file_name])\n",
    "    return json_file_name\n",
    "\n",
    "# helpful function for processing keywords, mostly    \n",
    "def getMultiples(items, key):\n",
    "    values_list = \"\"\n",
    "    if len(items) > 0:\n",
    "        num_keys = 0\n",
    "        for item in items:\n",
    "            if num_keys == 0:\n",
    "                values_list = item[key]                \n",
    "            else:\n",
    "                values_list =  \"; \".join([values_list,item[key]])\n",
    "            num_keys += 1\n",
    "    return values_list\n",
    "    \n",
    "# get the articles from the NYTimes Article API    \n",
    "def getArticles(start_date, end_date, query, api_key, json_file_path):\n",
    "    # LOOP THROUGH THE 101 PAGES NYTIMES ALLOWS FOR THAT DATE\n",
    "    for page in range(101):\n",
    "        for n in range(5): # 5 tries\n",
    "            try:\n",
    "                request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\" + query + \"&begin_date=\" + start_date + \"&end_date=\" + end_date + \"&page=\" + str(page) + \"&api-key=\" + api_key\n",
    "                response = urllib2.urlopen(request_string)\n",
    "                content = response.read()\n",
    "                if content:\n",
    "                    articles = convert(json.loads(content))\n",
    "                    # if there are articles here\n",
    "                    if len(articles[\"response\"][\"docs\"]) >= 1:\n",
    "                        json_file_name = getJsonFileName(start_date, page, json_file_path, query)\n",
    "                        json_file = open(json_file_name, 'w')\n",
    "                        json_file.write(content)\n",
    "                        json_file.close()\n",
    "                    # if no more articles, go to next date\n",
    "                    else:\n",
    "                        return\n",
    "                time.sleep(3) # wait so we don't overwhelm the API\n",
    "            except HTTPError as e:\n",
    "                logging.error(\"HTTPError on page %s on %s (err no. %s: %s) Here's the URL of the call: %s\", page, start_date, e.code, e.reason, request_string)\n",
    "                if e.code == 403:\n",
    "                    print \"Script hit a snag and got an HTTPError 403. Check your log file for more info.\"\n",
    "                    return\n",
    "                if e.code == 429:\n",
    "                    print \"Waiting. You've probably reached an API limit.\"\n",
    "                    time.sleep(30) # wait 30 seconds and try again\n",
    "            except: \n",
    "                logging.error(\"Error on %s page %s: %s\", start_date, file_number, sys.exc_info()[0])\n",
    "                continue\n",
    "        \n",
    "# Main function where stuff gets done\n",
    "def main():\n",
    "    log_file = \"NewsData/logfile.log\"\n",
    "    api_key = \"67736f1f941b44dc95f63619f6bdc9fb\"\n",
    "    start = datetime.date(year=2018, month=4, day=4)\n",
    "    end = datetime.date(year=2018,month=4, day=4) \n",
    "    query = 'immigration'\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "    \n",
    "    logging.info(\"Getting started.\") \n",
    "    try:\n",
    "        # LOOP THROUGH THE SPECIFIED DATES\n",
    "        for date in daterange(start, end):\n",
    "            start_date = date.strftime(\"%Y%m%d\")\n",
    "            print start_date            \n",
    "            date += datetime.timedelta(days=1)\n",
    "            end_date = date.strftime(\"%Y%m%d\")\n",
    "            print end_date\n",
    "            logging.info(\"Working on %s.\" % start_date)\n",
    "            json_file_path = \"NewsData/\"\n",
    "            try:\n",
    "                if not os.path.exists(os.path.dirname(json_file_path)):\n",
    "                    os.makedirs(os.path.dirname(json_file_path))\n",
    "            except OSError as err:\n",
    "                print(err)\n",
    "            getArticles(start_date, end_date, query, api_key, json_file_path)\n",
    "    except:\n",
    "        logging.error(\"Unexpected error: %s\", str(sys.exc_info()[0]))\n",
    "    finally:\n",
    "        logging.info(\"Finished.\")\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge 1\n",
      "breakthrough 1\n",
      "brexit 1\n",
      "vote 1\n",
      "british 1\n",
      "public 1\n",
      "utterly 1\n",
      "fed 1\n",
      "… 1\n",
      "https://t.co/mrb1ka0rh5 1\n",
      "build 1\n",
      "wall 1\n",
      "deport 1\n",
      "every 1\n",
      "illegal 1\n",
      "prisons 1\n",
      "end 1\n",
      "chain 1\n",
      "migration 1\n",
      "make 1\n",
      "english 1\n",
      "official 1\n",
      "language 1\n",
      "us 1\n",
      "… 1\n",
      "oakland 1\n",
      "mayor 1\n",
      "consulted 1\n",
      "illegal-immigration 1\n",
      "activists 1\n",
      "tipping 1\n",
      "ice 1\n",
      "raid 1\n",
      "https://t.co/pw9kaxl4zm 1\n",
      "criminal 1\n",
      "alien 1\n",
      "defending 1\n",
      "oakland 1\n",
      "mayor 1\n",
      "consulted 1\n",
      "illegal-immigration 1\n",
      "activists 1\n",
      "warning 1\n",
      "ice 1\n",
      "raids 1\n",
      "https://t.co/deillocopf 1\n",
      "39 1\n",
      "arrests 1\n",
      "crackdown 1\n",
      "albanian 1\n",
      "crime 1\n",
      "rings 1\n",
      "facilitating 1\n",
      "illegal 1\n",
      "immigration 1\n",
      "https://t.co/tprcop1rim 1\n"
     ]
    }
   ],
   "source": [
    "'''mapper.py'''\n",
    "import os, json, sys\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import operator \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "def get_json_files(path_to_json):\n",
    "    tweet_files = []\n",
    "    json_path = \"\".join([path_to_json, \"/\"])\n",
    "    tweet_files.append([str(json_path + pos_json) for pos_json in os.listdir(json_path) if pos_json.endswith('.json')])\n",
    "    return tweet_files\n",
    "\n",
    "\n",
    "def read_json_files(tweet_files):\n",
    "    tweets = []\n",
    "    for tweet_file in tweet_files:\n",
    "        for fil in tweet_file:\n",
    "            with open(str(fil), 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    tweets.append(json.loads(line))\n",
    "    return tweets\n",
    "\n",
    "def read_input(file):\n",
    "    tweets = []\n",
    "    for line in file:\n",
    "        tweets.append(json.loads(line))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop]\n",
    "    return filtered_tokens\n",
    "\n",
    "def mapper(tweets):\n",
    "    tokens = []\n",
    "    terms_only = []\n",
    "    d = []\n",
    "    for tweet in tweets[:5]:\n",
    "        tokens.extend(preprocess(tweet['text']))\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only.extend([term for term in preprocess(tweet['text']) if not term.startswith(('#', '@'))])\n",
    "    for term in terms_only:\n",
    "        d.append((term.encode('utf-8'), 1))\n",
    "    return d\n",
    "\n",
    "def main():\n",
    "#     tweet_files = get_json_files('TwitterData/')\n",
    "#     tweets = read_json_files(tweet_files)\n",
    "    tweets = read_input(sys.stdin)\n",
    "    d = mapper(tweets)\n",
    "    print '\\n'.join('%s %d' % val for val in d)\n",
    "#     with open('mapper-output.txt', 'w') as fp:\n",
    "#         fp.write('\\n'.join('%s %d' % val for val in d))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reducer.py'''\n",
    "import json\n",
    "\n",
    "def reducer(file_name):\n",
    "    if os.path.isfile(file_name):\n",
    "        in_file = open(file_name, 'r')\n",
    "        d = {}\n",
    "        for line in in_file: \n",
    "            line = line.strip()\n",
    "            word, count = line.split(\" \", 1)\n",
    "            try:\n",
    "                d[word] += count\n",
    "            except:\n",
    "                d[word] = count\n",
    "        in_file.close()\n",
    "        s = json.dumps([{'term': word, 'freq': count} for word, count in d.items()], indent = 4)\n",
    "        print s\n",
    "#         with open('result.json', 'w') as f:\n",
    "#             f.write(s)\n",
    "            \n",
    "def main():\n",
    "    reducer(sys.argv[1])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mapper-co.py'''\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import operator \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import bigrams \n",
    "from collections import defaultdict\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "def get_json_files(path_to_json):\n",
    "    tweet_files = []\n",
    "    json_path = \"\".join([path_to_json, \"/\"])\n",
    "    tweet_files.append([str(json_path + pos_json) for pos_json in os.listdir(json_path) if pos_json.endswith('.json')])\n",
    "    return tweet_files\n",
    "\n",
    "\n",
    "def read_json_files(tweet_files):\n",
    "    tweets = []\n",
    "    for tweet_file in tweet_files:\n",
    "        for fil in tweet_file:\n",
    "            with open(str(fil), 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    tweets.append(json.loads(line))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop]\n",
    "    return filtered_tokens\n",
    "\n",
    "def mapper(tweets):\n",
    "    tokens = []\n",
    "    terms_only = []\n",
    "    com = defaultdict(lambda : defaultdict(int))\n",
    "    for tweet in tweets[:5]:\n",
    "        tokens.extend(preprocess(tweet['text']))\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only.extend([term for term in preprocess(tweet['text']) if not term.startswith(('#', '@'))])\n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i].encode('utf-8'), terms_only[j].encode('utf-8')])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1\n",
    "    return com\n",
    "\n",
    "def main():\n",
    "    tweet_files = get_json_files(sys.argv[1])\n",
    "    tweets = read_json_files(tweet_files)\n",
    "    com = mapper(tweets)\n",
    "#     with open('mapper-output-co.txt', 'w') as fp:\n",
    "    for k,v in com.iteritems():   \n",
    "        for key,val in v.items():\n",
    "            t = (k, (key, val))\n",
    "            print '{} {} {}'.format(t[0], t[1][0], t[1][1])\n",
    "#                 fp.write('{} {} {}'.format(t[0], t[1][0], t[1][1]))\n",
    "#                 fp.write('\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('activists', 'illegal-immigration'), 4), (('activists', 'consulted'), 4), (('activists', 'oakland'), 4), (('activists', 'mayor'), 4), (('activists', 'illegal'), 4), (('activists', 'ice'), 4), (('activists', '\\xe2\\x80\\xa6'), 4), (('illegal-immigration', '\\xe2\\x80\\xa6'), 4), (('illegal-immigration', 'oakland'), 4), (('illegal-immigration', 'mayor'), 4), (('consulted', 'illegal-immigration'), 4), (('consulted', 'oakland'), 4), (('consulted', 'mayor'), 4), (('consulted', 'illegal'), 4), (('consulted', 'ice'), 4), (('consulted', '\\xe2\\x80\\xa6'), 4), (('oakland', '\\xe2\\x80\\xa6'), 4), (('mayor', '\\xe2\\x80\\xa6'), 4), (('mayor', 'oakland'), 4), (('illegal', 'illegal-immigration'), 4), (('illegal', '\\xe2\\x80\\xa6'), 4), (('illegal', 'oakland'), 4), (('illegal', 'mayor'), 4), (('ice', 'illegal-immigration'), 4), (('ice', 'illegal'), 4), (('ice', '\\xe2\\x80\\xa6'), 4), (('ice', 'oakland'), 4), (('ice', 'mayor'), 4), (('facilitating', 'illegal-immigration'), 2), (('facilitating', 'oakland'), 2), (('facilitating', 'mayor'), 2), (('facilitating', 'illegal'), 2), (('facilitating', 'ice'), 2), (('facilitating', '\\xe2\\x80\\xa6'), 2), (('chain', 'illegal-immigration'), 2), (('chain', 'consulted'), 2), (('chain', 'oakland'), 2), (('chain', 'mayor'), 2), (('chain', 'illegal'), 2), (('chain', 'ice'), 2), (('chain', '\\xe2\\x80\\xa6'), 2), (('wall', '\\xe2\\x80\\xa6'), 2), (('activists', 'official'), 2), (('activists', 'chain'), 2), (('activists', 'wall'), 2), (('illegal-immigration', 'raid'), 2), (('illegal-immigration', 'utterly'), 2), (('illegal-immigration', 'language'), 2), (('illegal-immigration', 'prisons'), 2), (('illegal-immigration', 'wall'), 2), (('illegal-immigration', 'make'), 2), (('illegal-immigration', 'official'), 2), (('alien', 'illegal-immigration'), 2), (('alien', 'consulted'), 2), (('alien', 'oakland'), 2), (('alien', 'mayor'), 2), (('alien', 'illegal'), 2), (('alien', 'ice'), 2), (('alien', '\\xe2\\x80\\xa6'), 2), (('arrests', 'illegal-immigration'), 2), (('arrests', 'consulted'), 2), (('arrests', 'oakland'), 2), (('arrests', 'mayor'), 2), (('arrests', 'illegal'), 2), (('arrests', 'ice'), 2), (('arrests', '\\xe2\\x80\\xa6'), 2), (('consulted', 'official'), 2), (('consulted', 'wall'), 2), (('consulted', 'warning'), 2), (('consulted', 'migration'), 2), (('warning', '\\xe2\\x80\\xa6'), 2), (('defending', 'illegal-immigration'), 2), (('defending', 'oakland'), 2), (('defending', 'mayor'), 2), (('defending', 'illegal'), 2), (('defending', 'ice'), 2), (('defending', '\\xe2\\x80\\xa6'), 2), (('vote', '\\xe2\\x80\\xa6'), 2), (('oakland', 'raid'), 2), (('oakland', 'utterly'), 2), (('oakland', 'prisons'), 2), (('oakland', 'wall'), 2), (('oakland', 'official'), 2), (('oakland', 'rings'), 2), (('oakland', 'us'), 2), (('oakland', 'warning'), 2), (('oakland', 'raids'), 2), (('mayor', 'raid'), 2), (('mayor', 'utterly'), 2), (('mayor', 'prisons'), 2), (('mayor', 'wall'), 2), (('mayor', 'official'), 2), (('mayor', 'rings'), 2), (('mayor', 'us'), 2), (('mayor', 'warning'), 2), (('huge', 'illegal-immigration'), 2), (('huge', 'oakland'), 2), (('huge', 'mayor'), 2), (('huge', 'illegal'), 2), (('huge', 'ice'), 2), (('huge', '\\xe2\\x80\\xa6'), 2), (('brexit', 'illegal-immigration'), 2), (('brexit', 'consulted'), 2), (('brexit', 'oakland'), 2), (('brexit', 'mayor'), 2), (('brexit', 'illegal'), 2), (('brexit', 'ice'), 2), (('brexit', '\\xe2\\x80\\xa6'), 2), (('end', 'illegal-immigration'), 2), (('end', 'oakland'), 2), (('end', 'mayor'), 2), (('end', 'illegal'), 2), (('end', 'ice'), 2), (('end', '\\xe2\\x80\\xa6'), 2), (('prisons', '\\xe2\\x80\\xa6'), 2), (('illegal', 'raid'), 2), (('illegal', 'utterly'), 2), (('illegal', 'language'), 2), (('illegal', 'prisons'), 2), (('illegal', 'wall'), 2), (('illegal', 'make'), 2), (('make', '\\xe2\\x80\\xa6'), 2), (('make', 'oakland'), 2), (('make', 'mayor'), 2), (('rings', '\\xe2\\x80\\xa6'), 2), (('ice', 'raid'), 2), (('ice', 'utterly'), 2), (('ice', 'language'), 2), (('ice', 'prisons'), 2), (('ice', 'wall'), 2), (('crime', 'illegal-immigration'), 2), (('crime', 'oakland'), 2), (('crime', 'mayor'), 2), (('crime', 'illegal'), 2), (('crime', 'ice'), 2), (('crime', '\\xe2\\x80\\xa6'), 2), (('crackdown', 'illegal-immigration'), 2), (('crackdown', 'oakland'), 2), (('crackdown', 'mayor'), 2), (('crackdown', 'illegal'), 2), (('crackdown', 'ice'), 2), (('crackdown', '\\xe2\\x80\\xa6'), 2), (('build', 'illegal-immigration'), 2), (('build', 'consulted'), 2), (('build', 'oakland'), 2), (('build', 'mayor'), 2), (('build', 'illegal'), 2), (('build', 'ice'), 2), (('build', '\\xe2\\x80\\xa6'), 2), (('https://t.co/deillocopf', 'illegal-immigration'), 2), (('https://t.co/deillocopf', 'oakland'), 2), (('https://t.co/deillocopf', 'mayor'), 2), (('https://t.co/deillocopf', 'illegal'), 2), (('https://t.co/deillocopf', 'ice'), 2), (('https://t.co/deillocopf', '\\xe2\\x80\\xa6'), 2), (('criminal', 'illegal-immigration'), 2), (('criminal', 'oakland'), 2), (('criminal', 'mayor'), 2), (('criminal', 'illegal'), 2), (('criminal', 'ice'), 2), (('criminal', '\\xe2\\x80\\xa6'), 2), (('public', '\\xe2\\x80\\xa6'), 2), (('british', 'illegal-immigration'), 2), (('british', 'consulted'), 2), (('british', 'oakland'), 2), (('british', 'mayor'), 2), (('british', 'illegal'), 2), (('british', 'ice'), 2), (('british', '\\xe2\\x80\\xa6'), 2), (('raid', '\\xe2\\x80\\xa6'), 2), (('raids', '\\xe2\\x80\\xa6'), 2), (('fed', 'illegal-immigration'), 2), (('fed', 'oakland'), 2), (('fed', 'mayor'), 2), (('fed', 'illegal'), 2), (('fed', 'ice'), 2), (('fed', '\\xe2\\x80\\xa6'), 2), (('immigration', '\\xe2\\x80\\xa6'), 2), (('immigration', 'oakland'), 2), (('immigration', 'mayor'), 2), (('every', 'illegal-immigration'), 2), (('every', 'oakland'), 2), (('every', 'mayor'), 2), (('every', 'illegal'), 2), (('every', 'ice'), 2), (('every', '\\xe2\\x80\\xa6'), 2), (('utterly', '\\xe2\\x80\\xa6'), 2), (('deport', 'illegal-immigration'), 2), (('deport', 'oakland'), 2), (('deport', 'mayor'), 2), (('deport', 'illegal'), 2), (('deport', 'ice'), 2), (('deport', '\\xe2\\x80\\xa6'), 2), (('breakthrough', 'illegal-immigration'), 2), (('breakthrough', 'consulted'), 2), (('breakthrough', 'oakland'), 2), (('breakthrough', 'mayor'), 2), (('breakthrough', 'illegal'), 2), (('breakthrough', 'ice'), 2), (('breakthrough', '\\xe2\\x80\\xa6'), 2), (('39', 'activists'), 2), (('39', 'illegal-immigration'), 2), (('39', 'consulted'), 2), (('39', 'oakland'), 2), (('39', 'mayor'), 2), (('39', 'illegal'), 2), (('39', 'ice'), 2), (('39', '\\xe2\\x80\\xa6'), 2), (('language', '\\xe2\\x80\\xa6'), 2), (('language', 'oakland'), 2), (('language', 'mayor'), 2), (('albanian', 'illegal-immigration'), 2), (('albanian', 'consulted'), 2), (('albanian', 'oakland'), 2), (('albanian', 'mayor'), 2), (('albanian', 'illegal'), 2), (('albanian', 'ice'), 2), (('albanian', '\\xe2\\x80\\xa6'), 2), (('official', '\\xe2\\x80\\xa6'), 2), (('us', '\\xe2\\x80\\xa6'), 2), (('https://t.co/tprcop1rim', 'illegal-immigration'), 2), (('https://t.co/tprcop1rim', 'oakland'), 2), (('https://t.co/tprcop1rim', 'mayor'), 2), (('https://t.co/tprcop1rim', 'illegal'), 2), (('https://t.co/tprcop1rim', 'ice'), 2), (('https://t.co/tprcop1rim', '\\xe2\\x80\\xa6'), 2), (('english', 'illegal-immigration'), 2), (('english', 'oakland'), 2), (('english', 'mayor'), 2), (('english', 'illegal'), 2), (('english', 'ice'), 2), (('english', '\\xe2\\x80\\xa6'), 2), (('https://t.co/pw9kaxl4zm', 'illegal-immigration'), 2), (('https://t.co/pw9kaxl4zm', 'oakland'), 2), (('https://t.co/pw9kaxl4zm', 'mayor'), 2), (('https://t.co/pw9kaxl4zm', 'illegal'), 2), (('https://t.co/pw9kaxl4zm', 'ice'), 2), (('https://t.co/pw9kaxl4zm', '\\xe2\\x80\\xa6'), 2), (('https://t.co/mrb1ka0rh5', 'illegal-immigration'), 2), (('https://t.co/mrb1ka0rh5', 'oakland'), 2), (('https://t.co/mrb1ka0rh5', 'mayor'), 2), (('https://t.co/mrb1ka0rh5', 'illegal'), 2), (('https://t.co/mrb1ka0rh5', 'ice'), 2), (('https://t.co/mrb1ka0rh5', '\\xe2\\x80\\xa6'), 2), (('tipping', '\\xe2\\x80\\xa6'), 2), (('migration', '\\xe2\\x80\\xa6'), 2), (('migration', 'oakland'), 2), (('facilitating', 'wall'), 1), (('facilitating', 'warning'), 1), (('facilitating', 'migration'), 1), (('facilitating', 'vote'), 1), (('chain', 'official'), 1), (('chain', 'wall'), 1), (('chain', 'warning'), 1), (('wall', 'warning'), 1), (('alien', 'official'), 1), (('alien', 'chain'), 1), (('alien', 'wall'), 1), (('arrests', 'official'), 1), (('arrests', 'chain'), 1), (('arrests', 'wall'), 1), (('defending', 'official'), 1), (('defending', 'wall'), 1), (('defending', 'warning'), 1), (('defending', 'migration'), 1), (('vote', 'wall'), 1), (('vote', 'warning'), 1), (('huge', 'wall'), 1), (('huge', 'warning'), 1), (('huge', 'migration'), 1), (('huge', 'vote'), 1), (('brexit', 'official'), 1), (('brexit', 'chain'), 1), (('brexit', 'wall'), 1), (('end', 'official'), 1), (('end', 'wall'), 1), (('end', 'warning'), 1), (('end', 'migration'), 1), (('prisons', 'raid'), 1), (('prisons', 'utterly'), 1), (('prisons', 'wall'), 1), (('prisons', 'rings'), 1), (('prisons', 'us'), 1), (('prisons', 'warning'), 1), (('prisons', 'raids'), 1), (('prisons', 'vote'), 1), (('prisons', 'tipping'), 1), (('make', 'raid'), 1), (('make', 'utterly'), 1), (('make', 'prisons'), 1), (('make', 'wall'), 1), (('make', 'official'), 1), (('make', 'rings'), 1), (('make', 'us'), 1), (('rings', 'wall'), 1), (('rings', 'us'), 1), (('rings', 'warning'), 1), (('rings', 'utterly'), 1), (('rings', 'vote'), 1), (('rings', 'tipping'), 1), (('crime', 'official'), 1), (('crime', 'wall'), 1), (('crime', 'warning'), 1), (('crime', 'migration'), 1), (('crackdown', 'official'), 1), (('crackdown', 'wall'), 1), (('crackdown', 'warning'), 1), (('crackdown', 'migration'), 1), (('build', 'official'), 1), (('build', 'chain'), 1), (('build', 'wall'), 1), (('https://t.co/deillocopf', 'wall'), 1), (('https://t.co/deillocopf', 'warning'), 1), (('https://t.co/deillocopf', 'migration'), 1), (('https://t.co/deillocopf', 'vote'), 1), (('criminal', 'official'), 1), (('criminal', 'wall'), 1), (('criminal', 'warning'), 1), (('criminal', 'migration'), 1), (('public', 'raid'), 1), (('public', 'utterly'), 1), (('public', 'wall'), 1), (('public', 'rings'), 1), (('public', 'us'), 1), (('public', 'warning'), 1), (('public', 'raids'), 1), (('public', 'vote'), 1), (('public', 'tipping'), 1), (('british', 'official'), 1), (('british', 'chain'), 1), (('british', 'wall'), 1), (('raid', 'utterly'), 1), (('raid', 'wall'), 1), (('raid', 'rings'), 1), (('raid', 'us'), 1), (('raid', 'warning'), 1), (('raid', 'raids'), 1), (('raid', 'vote'), 1), (('raid', 'tipping'), 1), (('raids', 'wall'), 1), (('raids', 'rings'), 1), (('raids', 'us'), 1), (('raids', 'warning'), 1), (('raids', 'utterly'), 1), (('raids', 'vote'), 1), (('raids', 'tipping'), 1), (('fed', 'wall'), 1), (('fed', 'warning'), 1), (('fed', 'migration'), 1), (('fed', 'vote'), 1), (('immigration', 'raid'), 1), (('immigration', 'utterly'), 1), (('immigration', 'language'), 1), (('immigration', 'prisons'), 1), (('immigration', 'wall'), 1), (('immigration', 'make'), 1), (('immigration', 'official'), 1), (('every', 'official'), 1), (('every', 'wall'), 1), (('every', 'warning'), 1), (('every', 'migration'), 1), (('utterly', 'vote'), 1), (('utterly', 'wall'), 1), (('utterly', 'warning'), 1), (('deport', 'official'), 1), (('deport', 'wall'), 1), (('deport', 'warning'), 1), (('deport', 'migration'), 1), (('breakthrough', 'official'), 1), (('breakthrough', 'chain'), 1), (('breakthrough', 'wall'), 1), (('39', 'official'), 1), (('39', 'chain'), 1), (('language', 'raid'), 1), (('language', 'utterly'), 1), (('language', 'prisons'), 1), (('language', 'wall'), 1), (('language', 'make'), 1), (('language', 'official'), 1), (('language', 'rings'), 1), (('albanian', 'official'), 1), (('albanian', 'chain'), 1), (('albanian', 'wall'), 1), (('official', 'raid'), 1), (('official', 'utterly'), 1), (('official', 'prisons'), 1), (('official', 'wall'), 1), (('official', 'rings'), 1), (('official', 'us'), 1), (('official', 'warning'), 1), (('official', 'raids'), 1), (('official', 'vote'), 1), (('us', 'vote'), 1), (('us', 'wall'), 1), (('us', 'warning'), 1), (('us', 'utterly'), 1), (('https://t.co/tprcop1rim', 'wall'), 1), (('https://t.co/tprcop1rim', 'warning'), 1), (('https://t.co/tprcop1rim', 'migration'), 1), (('https://t.co/tprcop1rim', 'vote'), 1), (('english', 'official'), 1), (('english', 'wall'), 1), (('english', 'warning'), 1), (('english', 'migration'), 1), (('https://t.co/pw9kaxl4zm', 'wall'), 1), (('https://t.co/pw9kaxl4zm', 'warning'), 1), (('https://t.co/pw9kaxl4zm', 'migration'), 1), (('https://t.co/pw9kaxl4zm', 'vote'), 1), (('https://t.co/mrb1ka0rh5', 'wall'), 1), (('https://t.co/mrb1ka0rh5', 'warning'), 1), (('https://t.co/mrb1ka0rh5', 'migration'), 1), (('https://t.co/mrb1ka0rh5', 'vote'), 1), (('tipping', 'wall'), 1), (('tipping', 'us'), 1), (('tipping', 'warning'), 1), (('tipping', 'utterly'), 1), (('tipping', 'vote'), 1), (('migration', 'raid'), 1), (('migration', 'utterly'), 1), (('migration', 'prisons'), 1), (('migration', 'wall'), 1), (('migration', 'official'), 1), (('migration', 'rings'), 1), (('migration', 'us'), 1), (('migration', 'warning'), 1)]\n"
     ]
    }
   ],
   "source": [
    "'''reducer-co.py'''\n",
    "import json\n",
    "\n",
    "def reducer(file_name):\n",
    "    if os.path.isfile(file_name):\n",
    "        in_file = open(file_name, 'r')\n",
    "        co_oc = defaultdict(lambda : defaultdict(int))\n",
    "        for line in in_file: \n",
    "            line = line.strip()\n",
    "            word, co_word, count = line.split(\" \", 2)\n",
    "            co_oc[word][co_word] += int(count)\n",
    "        in_file.close()\n",
    "        com_max = []\n",
    "        # For each term, look for the most common co-occurrent terms\n",
    "        for t1 in co_oc:\n",
    "            t1_max_terms = sorted(co_oc[t1].items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "            for t2, t2_count in t1_max_terms:\n",
    "                com_max.append(((t1, t2), t2_count))\n",
    "        # Get the most frequent co-occurrences\n",
    "        terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "#         with open('reducer-co.txt', 'w') as fp:\n",
    "        for t in terms_max[:10]:\n",
    "            print '{} {} {}'.format(t[0][0], t[0][1], t[1])\n",
    "#                 fp.write('{} {} {}'.format(t[0][0], t[0][1], t[1]))\n",
    "#                 fp.write('\\n')\n",
    "\n",
    "def main():\n",
    "    reducer(sys.argv[1])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the JSON files you stored into a comma-delimited file\n",
    "def parseArticles(start_date, end_date, query, csv_file_name, json_file_path):\n",
    "    for file_number in range(101):\n",
    "        # get the articles and put them into a dictionary\n",
    "        try:\n",
    "            file_name = getJsonFileName(start_date, file_number, json_file_path, query)\n",
    "            if os.path.isfile(file_name):\n",
    "                in_file = open(file_name, 'r')\n",
    "                articles = convert(json.loads(in_file.read()))\n",
    "                in_file.close()\n",
    "            else:\n",
    "                break\n",
    "        except IOError as e:\n",
    "            logging.error(\"IOError in %s page %s: %s %s\", start_date, file_number, e.errno, e.strerror)\n",
    "            continue\n",
    "        \n",
    "        # if there are articles in that document, parse them\n",
    "        if len(articles[\"response\"][\"docs\"]) >= 1:  \n",
    "\n",
    "            # open the csv for appending\n",
    "            try:\n",
    "                out_file = open(csv_file_name, 'ab')\n",
    "\n",
    "            except IOError as e:\n",
    "                logging.error(\"IOError: %s %s %s %s\", start_date, file_number, e.errno, e.strerror)\n",
    "                continue\n",
    "        \n",
    "            # loop through the articles putting what we need in a tsv   \n",
    "            try:\n",
    "                for article in articles[\"response\"][\"docs\"]:\n",
    "                    # if (article[\"source\"] == \"The New York Times\" and article[\"document_type\"] == \"article\"):\n",
    "                    keywords = \"\"\n",
    "                    keywords = getMultiples(article[\"keywords\"],\"value\")\n",
    "    \n",
    "                    # should probably pull these if/else checks into a module\n",
    "                    variables = [\n",
    "                        article[\"pub_date\"], \n",
    "                        keywords, \n",
    "                        str(article[\"headline\"][\"main\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"main\" in article[\"headline\"].keys() else \"\", \n",
    "                        str(article[\"source\"]).decode(\"utf8\") if \"source\" in article.keys() else \"\", \n",
    "                        str(article[\"document_type\"]).decode(\"utf8\") if \"document_type\" in article.keys() else \"\", \n",
    "                        article[\"web_url\"] if \"web_url\" in article.keys() else \"\",\n",
    "                        str(article[\"news_desk\"]).decode(\"utf8\") if \"news_desk\" in article.keys() else \"\",\n",
    "                        str(article[\"section_name\"]).decode(\"utf8\") if \"section_name\" in article.keys() else \"\",\n",
    "                        str(article[\"snippet\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"snippet\" in article.keys() else \"\",\n",
    "                        str(article[\"lead_paragraph\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"lead_paragraph\" in article.keys() else \"\",\n",
    "                        ]\n",
    "                    line = \",\".join(variables)\n",
    "                    out_file.write(line.encode(\"utf8\")+\"\\n\")\n",
    "            except KeyError as e:\n",
    "                logging.error(\"KeyError in %s page %s: %s %s\", start_date, file_number, e.errno, e.strerror)\n",
    "                continue\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except: \n",
    "                logging.error(\"Error on %s page %s: %s\", start_date, file_number, sys.exc_info()[0])\n",
    "                continue\n",
    "        \n",
    "            out_file.close()\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mapper.py'''\n",
    "import os, json, sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "    [:=;] # Eyes\n",
    "    [oO\\-]? # Nose (optional)\n",
    "    [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "             emoticons_str,\n",
    "             r'<[^>]+>', # HTML tags\n",
    "             r'(?:@[\\w_]+)', # @-mentions\n",
    "             r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "             r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "             r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "             r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "             r'(?:[\\w_]+)', # other words\n",
    "             r'(?:\\S)' # anything else\n",
    "             ]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "def read_input(file):\n",
    "    articles = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            articles.append(json.loads(line))\n",
    "    return articles\n",
    "\n",
    "def parse_articles(articles):\n",
    "    news = []\n",
    "    print articles\n",
    "    if len(articles) >= 1:  \n",
    "        print articles\n",
    "        for article in articles:\n",
    "            for art in article[\"response\"][\"docs\"]:\n",
    "                # if (article[\"source\"] == \"The New York Times\" and article[\"document_type\"] == \"article\"):\n",
    "                keywords = \"\"\n",
    "                keywords = getMultiples(art[\"keywords\"],\"value\")\n",
    "                variables = str(art[\"snippet\"]) if \"snippet\" in art.keys() else \"\"\n",
    "                print variables\n",
    "#         news.append(dic)\n",
    "    return(news) \n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop]\n",
    "    return filtered_tokens\n",
    "\n",
    "def mapper(tweets):\n",
    "    tokens = []\n",
    "    d = []\n",
    "    for tweet in tweets:\n",
    "        print tweet\n",
    "#         tokens.extend(preprocess(tweet['snippet']))\n",
    "#     for term in tokens:\n",
    "#         d.append((term.encode('utf-8'), 1))\n",
    "#     print tokens\n",
    "    return d\n",
    "\n",
    "def main():\n",
    "    articles = read_input(\"NewsData/immigration_20170721_0.json\")\n",
    "    news = parse_articles(articles)\n",
    "    print news\n",
    "#     d = mapper(news)\n",
    "#    for t in d:\n",
    "#        print t[0], t[1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "def get_json_files(path_to_json):\n",
    "    tweet_files = []\n",
    "    json_path = \"\".join([path_to_json, \"/\"])\n",
    "    tweet_files.append([str(json_path + pos_json) for pos_json in os.listdir(json_path) if pos_json.endswith('.json')])\n",
    "    return tweet_files\n",
    "\n",
    "\n",
    "def read_json_files(tweet_files):\n",
    "    tweets = []\n",
    "    for tweet_file in tweet_files:\n",
    "        for fil in tweet_file:\n",
    "            with open(str(fil), 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    tweets.extend(json.loads(line))\n",
    "    return tweets\n",
    "\n",
    "def main():\n",
    "    tweet_files = get_json_files(\"Data/\")\n",
    "    tweets = read_json_files(tweet_files)\n",
    "    d = {}\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            d[tweet[\"term\"]] += int(tweet[\"freq\"])\n",
    "        except:\n",
    "            d[tweet[\"term\"]] = int(tweet[\"freq\"])\n",
    "    s = json.dumps([{\"term\": word, \"freq\": count} for word, count in d.items()], indent = 4)\n",
    "    with open(\"result1.json\", \"w\") as f:\n",
    "        f.write(s)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def get_files(path):\n",
    "    tweet_files = []\n",
    "    json_path = \"\".join([path, \"/\"])\n",
    "    tweet_files.append([str(json_path + pos_json) for pos_json in os.listdir(json_path) if pos_json.endswith('.txt')])\n",
    "    return tweet_files\n",
    "\n",
    "\n",
    "def read_files(tweet_files):\n",
    "    co_oc = defaultdict(lambda : defaultdict(int))\n",
    "    for tweet_file in tweet_files:\n",
    "        for fil in tweet_file:\n",
    "            with open(str(fil), 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    word, co_word, count = line.split(\" \", 2)\n",
    "                    co_oc[word][co_word] += int(count)\n",
    "    com_max = []\n",
    "    # For each term, look for the most common co-occurrent terms\n",
    "    for t1 in co_oc:\n",
    "        t1_max_terms = sorted(co_oc[t1].items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "        for t2, t2_count in t1_max_terms:\n",
    "            com_max.append(((t1, t2), t2_count))\n",
    "    # Get the most frequent co-occurrences\n",
    "    terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "    with open('result2.json', 'w') as fp:\n",
    "        fp.write(\"[\")\n",
    "        for t in terms_max[:10]:\n",
    "            json.dump({'term': t[0][0], 'co_term': t[0][1], 'freq': t[1]}, fp)\n",
    "            fp.write(\",\\n\")\n",
    "        fp.write(\"]\")\n",
    "\n",
    "def main():\n",
    "    tweet_files = get_files(\"OutputNewsDataCo/\")\n",
    "    tweets = read_files(tweet_files)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
